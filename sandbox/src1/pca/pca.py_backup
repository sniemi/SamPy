#This script tests the PCA method
#2008 Nov, Sami-Matias Niemi

import numpy as N
import numpy.linalg as NA
import scipy as S
import scipy.linalg as SA

stdnorm = False

def pca(M):
    from Numeric import take, dot, shape, argsort, where, sqrt, transpose as t
    from LinearAlgebra import eigenvectors
    "Perform PCA on M, return eigenvectors and eigenvalues, sorted."
    T, N = shape(M)
    # if there are less rows T than columns N, use
    # snapshot method
    if T < N:
        C = dot(M, t(M))
        evals, evecsC = eigenvectors(C)
        # HACK: make sure evals are all positive
        evals = where(evals < 0, 0, evals)
        evecs = 1./sqrt(evals) * dot(t(M), t(evecsC))
    else:
        # calculate covariance matrix
        K = 1./T * dot(t(M), M)
        evals, evecs = eigenvectors(K)
    # sort the eigenvalues and eigenvectors, decending order
    order = (argsort(evals)[::-1])
    evecs = take(evecs, order, 1)
    evals = take(evals, order)
    return evals, t(evecs)


#data are arranged into an array
x = [2.5,0.5,2.2,1.9,3.1,2.3,2.0,1.0,1.5,1.1]
y = [2.4,0.7,2.9,2.2,3.0,2.7,1.6,1.1,1.6,0.9]
orgdata = N.transpose(N.array([x , y]))
#x = [2.5,0.5,2.2,1.9,3.1,2.3,2.0,1.0,1.5,1.1]
#y = [2.4,0.7,2.9,2.2,3.0,2.7,1.6,1.1,1.6,0.9]
#z = [1.2,3.4,1.2,5.6,3.0,0.1,3.7,3.3,2.2,2.2]
#orgdata = N.transpose(N.array([x , y, z]))


#Removes mean
avg = N.mean(orgdata, axis = 0)
#a PCA without dividing by the standard deviation is an eigenanalysis of the covariance matrix, 
#and a PCA in which you do indeed divide by the standard deviation is an eigenanalysis of the correlation matrix.
if stdnorm: data = (orgdata - avg) / N.std(orgdata, axis = 0)
else: data = orgdata - avg

print 'Original data transposed:\n', orgdata
print 

#Calculates the covariance matrix
#For matrices where each row is an observation, 
#and each column a variable, return the covariance matrix!
cov = N.cov(N.transpose(N.mat(data)))
#For 2dim:
#If the non-diagonal elements in the covariance matrix are positive, 
#we should expect that both the x and y variable increase together.

#Just a test...
#U, S_values, V_trans = SA.svd(data)

#Calculates eigenvalues and eigenvectors
evaltrue, evecorg = NA.eig(N.mat(cov))
evalorg = N.transpose(N.mat(N.abs(evaltrue)))
temp = NA.eig(N.mat(cov))

#Finds the principal component:
#sort eigenvectors by eigenvalues
tmp = []
for val, vec in zip(temp[0], temp[1]):
    tmp.append([val.real, vec])

dimensions = len(tmp)

#Gives the components in order of significance:
sorted = sorted(tmp, cmp = lambda x,y: (-1 if x[0] < y[0] else (1 if x[0] > y[0] else 0)), reverse = True)

eval = []
evec = []
for value in sorted:
    eval.append(value[0])
    evec.append(value[1])

eval = N.transpose(N.mat(eval))
evec = N.mat(evec)

featurevec = N.mat([[sorted[1][1][1], sorted[0][1][1]],
                    [sorted[1][1][0], sorted[0][1][0]]])

#the eigenvector with the highest eigenvalue is the principle component of the data set.
maxfeaturevec = featurevec[0][:]

#New Data
maxnewdata = N.mat(maxfeaturevec) * N.transpose(N.mat(data))
newdata = featurevec * N.transpose(N.mat(data))

if stdnorm:
    dim = N.sum(eval)
    perc = eval / dim * 100.
    i = 0
    for p in perc:
        i += 1
        #How much axis explains of the variation in the entire data set
        print 'PCA Axis %i: %4.2f per cent' % (i, p)

#First PC is direction of maximum variance from origin
#Subsequent PCs are orthogonal to 1st PC and describe maximum residual variance

#Eigenvector cells are coefficients of the corresponding principal component

#Gives some Output
print 'Covariance matrix:\n', cov
print
print 'Eigenvalues:\n', evalorg
print
#print 'Sorted Eigenvalues:\n', eval
#print
print 'Eigenvectors:\n', evecorg
print
#print 'Sorted Eigenvectors:\n', evec
#print
print 'Feature matrix:\n', featurevec
print
print 'Final Data Transposed:\n', N.transpose(N.mat(newdata))
print
print 'Major Component Data:\n', N.transpose(N.mat(maxnewdata))
print

import sys
sys.exit()

import pylab as P

newdata = N.array(newdata)

P.scatter(newdata[0,:], newdata[1,:])
P.axhline(0.)
P.axvline(0.)
P.show()
